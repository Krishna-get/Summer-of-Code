#  VHDL Hardware Acceleration for AI  
### A Summer of Science Project at IIT Bombay

This repository documents the complete journey of designing, implementing, and verifying a **learningâ€‘capable neural network accelerator** in **VHDL**. From foundational digital logic to a full training+inference engine, and capped with an architectural study of Googleâ€™s TPU v1, this project bridges academic design and industryâ€‘grade AI hardware.

---

## ğŸ“– Project Journey & Timeline

**Duration:** 8 weeks  
**Tools:** Intel Quartus Prime (synthesis), ModelSim (simulation)

| Weeks       | Focus Area                                            |
|-------------|-------------------------------------------------------|
| **1â€“2**     | VHDL Fundamentals & Combinational Logic               |
| **3â€“4**     | Sequential Logic & Finiteâ€‘State Machines (FSMs)       |
| **5â€“7**     | Building the Neural Network Accelerator (Inference + Training) |
| **8**       | Research & Industry Context (TPU v1 Architectural Study)       |

### Weeks 1â€“2: VHDL Fundamentals & Combinational Logic
- **Environment Setup**  
  â€“ Installed Intel Quartus Prime & ModelSim.  
- **Core Philosophy**  
  â€“ Built a universal NAND gate (`NAND_pkg.vhd`), then derived AND, OR, XOR, NOT.  
- **Combinational Circuits**  
  â€“ Full Adders/Subtractors â†’ 4â€‘bit Rippleâ€‘Carry Adders/Subtractors.  
  â€“ Multiplexers: 2â€‘toâ€‘1, 4â€‘toâ€‘1, 8â€‘toâ€‘1 MUXes.  
- **Verification**  
  â€“ Testbenches for each component; RTL schematic checks; waveform validation in ModelSim.

### Weeks 3â€“4: Sequential Logic & FSMs
- **Memory Elements**  
  â€“ D Flipâ€‘Flops â†’ 8â€‘bit registers.  
- **FSM Design**  
  â€“ Studied Mealy vs. Moore; chose Mealy for speed.  
  â€“ Implemented a sequence detector to reinforce stateâ€‘machine concepts.  
- **Verification**  
  â€“ Comprehensive testbenches; waveform analysis for state transitions and timing.

### Weeks 5â€“7: Building the Neural Network Accelerator
- **Phase 1 â€“ Forward Propagation (Inference)**  
  â€“ Designed `nn_controller` FSM with looping states (`S_MAC_H_LOOP`, `S_MAC_Y_LOOP`) and internal counters to handle multiâ€‘cycle dotâ€‘product calculations.
- **Phase 2 â€“ Backward Propagation (Training)**  
  â€“ Integrated `error_calculator` & `weight_updater`.  
  â€“ Extended FSM with `S_CALC_ERROR` and `S_UPDATE_WEIGHTS` states under a `train_mode` input.
- **Debugging Challenges**  
  â€“ Resolved VHDL syntax/type mismatches (SIGNED vs. STD_LOGIC_VECTOR).  
  â€“ Eliminated inferredâ€‘latch warnings by proper reset initialization.  
  â€“ Fixed simulation errors (â€œindex out of boundsâ€, â€œnot globally staticâ€) by refactoring generate loops into explicit instantiations.

### Week 8: Research & Industry Context
- **Architectural Study of Google TPU v1**  
  â€“ Explored systolic arrays, quantization strategies, and specialized memory hierarchies.  
  â€“ Compared performance/efficiency tradeâ€‘offs between academic design and TPU architecture.

---

## ğŸš€ Final Accelerator: Features & Architecture

- **Topology**: 3 inputs â†’ 4 hidden neurons â†’ 2 outputs  
- **Dualâ€‘Mode Operation**:  
  - **Inference** (fast forward pass)  
  - **Training** (full backpropagation & weight updates via gradient descent)  
- **Backpropagation Engine**: Hardware implementation of gradient descent  
- **Modular & Hierarchical**: Independently verified VHDL modules  
- **Mealy FSM**: Coordinates multiâ€‘cycle MAC and weightâ€‘update loops  

---

## ğŸ› ï¸ Hardware Modules

| Module                  | Function                                                                 |
|-------------------------|--------------------------------------------------------------------------|
| **nn_controller.vhd**   | Topâ€‘level FSM controller for inference & training                       |
| **mac_unit.vhd**        | Multiplyâ€‘Accumulate: `(input Ã— weight) + bias`                           |
| **relu_activation.vhd** | ReLU nonâ€‘linearity: `max(0, input)`                                      |
| **error_calculator.vhd**| Computes output error & activation derivative for backpropagation       |
| **weight_updater.vhd**  | Updates weights: `new_weight = old_weight âˆ’ (error Ã— learning_rate)`    |

---

## ğŸ’¡ How to Simulate the Final Design

1. **Set Topâ€‘Level Entity**  
   In Quartus: `nn_controller`

2. **Configure Testbench**  
   Use **`nn_controller_tb.vhd`**, which drives:  
clk, rst, start, input_data, target_output, train_mode

3. **Simulation Steps**  
- **Compile**: `Processing â†’ Start Compilation` (0 errors/warnings)  
- **Launch RTL Simulation**: `Tools â†’ Run Simulation Tool â†’ RTL Simulation`

4. **Waveform Verification** (ModelSim)  
Add the following signals from the `uut` instance:  
- `current_state` (observe inference â†’ backpropagation â†’ idle)  
- `h_mac_counter`, `y_mac_counter` (loop iteration counts)  
- `output_data` (inference result)  
- `W2`, `W2_new` (weight values before/after update)  
- `training_complete` (pulse indicating end of training cycle)

---
